# UHGG Genome Autoencoder Ranking

This project trains and evaluates autoencoders on metadata from the Unified Human Gastrointestinal Genome (UHGG)
collection to rank genomes based on their reconstruction error. Lower reconstruction error is interpreted as higher "
typicality" â€” indicating that the genome is better represented by patterns in the population. This
project was developed as a representative selection solution in Yassour Lab.

---

## ğŸ” Objective

- **Unsupervised quality/ranking** of genomes using metadata alone
- Discover "typical" vs. "atypical" genomes using autoencoder reconstruction error
- Benchmark different autoencoder architectures and hyperparameters
- Output per-genome scores, ranked lists, and reusable models

---

## ğŸ“ Input

The input is a **CSV file** containing metadata for each genome.
Each row represents a genome, and columns are numerical features such as:

- `Length`
- `N_contigs`
- `N50`
- `GC_content`
- `rRNA_5S`, `rRNA_16S`, `rRNA_23S`
- `tRNAs`

All features are automatically normalized using `StandardScaler`.

---

## ğŸš€ Usage

Run the main evaluation and training script with:

Arguments:

- `in_file`: Path to the metadata TSV file
- `model_path`: Path where the final trained autoencoder model will be saved

After evaluating multiple autoencoders and tuning the dropout rate, the final best model will be retrained on the full
data and saved to the provided output file. It should be a `.keras` file.

---

## ğŸ§  Autoencoder Overview

Each autoencoder:

- Encodes input metadata into a low-dimensional **latent space**
- Attempts to reconstruct the original input
- Computes **mean squared error (MSE)** per genome

A genome with **low reconstruction error** is considered **typical** of the training distribution.

---

## âš™ï¸ Evaluation Pipeline

1. **Train on a subset** of genomes (for speed)
2. Evaluate **multiple architectures** (e.g. `[64, 32]`, `[128, 64, 32]`, etc.)
3. Use **5-fold cross-validation** to estimate average reconstruction error
4. Plot performance and save evaluation metrics
5. **Fine-tune dropout rate** for best architecture
6. Retrain best model on the **full dataset**
7. Save the final model and per-genome scores

---

## ğŸ—ï¸ Architecture Options

Tested autoencoders vary by:

- Hidden layers (`[64]`, `[64, 32]`, `[128, 64, 32]`, etc.)
- Latent dimension (`4`, `8`, `16`)
- Activation function (default: ReLU)
- Dropout usage and rate (e.g., `0.0` to `0.4`)

Each architecture is evaluated for:

- **Mean reconstruction error (MSE)**
- **Standard deviation** across CV folds

---

## ğŸ“Š Output

Artifacts generated by the pipeline:

- `evaluation_results.csv`: MSE per architecture
- `autoencoder_comparison.png`: Bar plot of architectures vs. performance
- `dropout_tuning_results.csv`: Dropout tuning results
- `dropout_tuning_plot.png`: Dropout vs. performance
- `uhgg_with_reconstruction_mse_metadata.tsv`: Metadata with per-genome reconstruction MSE scores
---

## ğŸ“ˆ Interpretation

After training, each genome receives a **reconstruction MSE** score:

- Lower MSE â†’ genome closely matches latent patterns
- Higher MSE â†’ genome is anomalous or less represented

This ranking can help:

- Identify outliers
- Select representative genomes for clustering
- Pre-screen genomes for curation

---

## ğŸ›  Dependencies

- `tensorflow`
- `numpy`
- `pandas`
- `matplotlib`
- `scikit-learn`

---

## âœï¸ Author Notes

This is an exploratory unsupervised learning project aimed at genome metadata representation. Future improvements may
include:

- Using denoising autoencoders
- Testing variational autoencoders (VAEs)
- Integrating SHAP for feature attribution
- Combining autoencoder embeddings with MASH distances or k-mer embeddings